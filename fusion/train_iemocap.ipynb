{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g161cICVTY-0"
   },
   "source": [
    "## Prepare settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2254,
     "status": "ok",
     "timestamp": 1687508383078,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": -480
    },
    "id": "6mim_dahLXag",
    "outputId": "80c7ec7c-68b5-4a2f-f970-d7c14442afa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "# add path\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Code/LMF_attention_tt/ieamocap\")\n",
    "import sys;\n",
    "sys.path.append('/content/drive/MyDrive/Code/LMF_attention_tt/tensor_fusion')\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from model import LMF, SubNet\n",
    "from utils import load_iemocap\n",
    "from tensor_fusion.module import AdaptiveRankLinear\n",
    "from tensor_layers.Transformer_tensor import TextSubNet_attention\n",
    "from tensor_layers.layers import TensorizedLinear, TensorizedLinear_module\n",
    "\n",
    "# Enable this and writer ovject to track metrics during training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter(comment=comment, log_dir='./tt_runs')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "DTYPE = torch.FloatTensor\n",
    "LONG = torch.LongTensor\n",
    "\n",
    "def display(f1_score, accuracy_score):\n",
    "    print(\"F1-score on test set is {}\".format(f1_score))\n",
    "    print(\"Accuracy score on test set is {}\".format(accuracy_score))\n",
    "\n",
    "data_path = '/content/drive/MyDrive/data/'\n",
    "\n",
    "# emotion = 'neutral'\n",
    "output_dim = 2\n",
    "\n",
    "# Training parameters\n",
    "epochs = 125\n",
    "# epochs = 5\n",
    "model_path_dir = './tt_models/'\n",
    "signiture = 'lmf'\n",
    "patience = 50\n",
    "\n",
    "factor_lr = 0.003\n",
    "lr = 0.0005\n",
    "\n",
    "r = 8\n",
    "batch_sz = 64\n",
    "decay = 0.001\n",
    "cuda = 1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "complete = True\n",
    "\n",
    "curr_patience = patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNBkXWefLXan"
   },
   "source": [
    "## Model Structure Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "executionInfo": {
     "elapsed": 6342,
     "status": "error",
     "timestamp": 1687480800181,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": -480
    },
    "id": "6rCl3IMeLXaq",
    "outputId": "c7996867-e167-436b-fd8f-5c0c14520255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m input_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m36\u001b[39m,\u001b[38;5;241m300\u001b[39m,\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# def __init__(self, input_dims, hidden_dims, audio_out, video_out, text_out, n_layers, n_head, dropouts, output_dim, rank, max_rank, TT_FUSION, use_LSTM, tensor_type, device, dtype, use_softmax=False)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLMF\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mahid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvhid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43madr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvdr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtdr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTTM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m model\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 15 to 16 positional arguments but 17 were given"
     ]
    }
   ],
   "source": [
    "# Alex\n",
    "adr = 0.3\n",
    "vdr = 0.1\n",
    "tdr = 0.15\n",
    "\n",
    "ahid = 32\n",
    "vhid = 32\n",
    "thid = 64\n",
    "audio_out = ahid\n",
    "video_out = vhid\n",
    "text_out = thid\n",
    "n_layers = 2\n",
    "n_head   = 3\n",
    "\n",
    "emotion = 'happy'\n",
    "train_set, valid_set, test_set, data_dims = load_iemocap(data_path, emotion)\n",
    "train_iterator = DataLoader(train_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "test_iterator = DataLoader(test_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "\n",
    "input_dims = [80,36,300,20]\n",
    "# def __init__(self, input_dims, hidden_dims, audio_out, video_out, text_out, n_layers, n_head, dropouts, output_dim, rank, max_rank, TT_FUSION, use_LSTM, tensor_type, device, dtype, use_softmax=False)\n",
    "model = LMF(input_dims, (ahid, vhid, thid), audio_out, video_out, text_out, n_layers, n_head, (adr, vdr, tdr, 0.5), output_dim, 6, 4, 0, 1, 'TTM',device, dtype)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDAddCdzLKWY"
   },
   "outputs": [],
   "source": [
    "x = next(iter(train_iterator))\n",
    "x_a = Variable(x[0].float().type(DTYPE), requires_grad=False)\n",
    "x_v = Variable(x[1].float().type(DTYPE), requires_grad=False)\n",
    "x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "y = Variable(x[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "print(x_a[0])\n",
    "print(x_v[0])\n",
    "print(x_t[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKRziaeFLXar"
   },
   "source": [
    "## BP Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5926,
     "status": "ok",
     "timestamp": 1687480930856,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": -480
    },
    "id": "f0xVRtDDbrIE",
    "outputId": "7fee7d17-cf4c-4e1e-d9db-29c1373cb135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n",
      "Train batch: 43\n",
      "Valid batch: 13\n",
      "Test batch: 15\n"
     ]
    }
   ],
   "source": [
    "# trianing setting (execute if only find best)\n",
    "# rank_list = [3,4,5,6]\n",
    "\n",
    "signiture = 'attn'\n",
    "\n",
    "adr = 0.3\n",
    "vdr = 0.1\n",
    "tdr = 0.15\n",
    "fdr = 0.5\n",
    "dropouts = (adr, vdr, tdr, fdr)\n",
    "\n",
    "ahid = 32\n",
    "vhid = 32\n",
    "thid = 64\n",
    "ahid_list = [8,16,32]\n",
    "vhid_list = [4,8,16]\n",
    "thid_list = [128,256,64]\n",
    "\n",
    "audio_out = ahid\n",
    "video_out = vhid\n",
    "text_out = thid\n",
    "text_out_list = [7,15,23,31]\n",
    "\n",
    "output_dim = 2\n",
    "\n",
    "max_rank = 4\n",
    "max_rank_list = [2,4,6,8,10]\n",
    "\n",
    "fusion_rank = 8\n",
    "fusion_rank_list = [4,8]\n",
    "\n",
    "n_layers = 1\n",
    "n_layers_list = [1,2,3]\n",
    "n_layers_list = [1,2]\n",
    "\n",
    "n_head = 2\n",
    "n_head_list = [2,3,4,5,6]\n",
    "n_head_list = [2,3]\n",
    "\n",
    "TT_FUSION = 0\n",
    "TT_FUSION_list = [1, 0]\n",
    "\n",
    "use_LSTM = 0\n",
    "use_LSTM_list = [1, 0]\n",
    "\n",
    "lr = 0.001\n",
    "lr_list = [0.001, 0.003]\n",
    "# lr_list = [0.0003, 0.0005, 0.001, 0.003]\n",
    "\n",
    "decay = 0.001\n",
    "decay_list = [0, 0.002, 0.01]\n",
    "# decay_list = [0, 0.001, 0.002, 0.01]\n",
    "\n",
    "batch_sz = 64\n",
    "batch_sz_list = [16, 32, 64]\n",
    "\n",
    "emotion = 'happy'\n",
    "emotion_list = ['happy','sad','angry','neutral']\n",
    "\n",
    "train_set, valid_set, test_set, data_dims = load_iemocap(data_path, emotion)\n",
    "train_iterator = DataLoader(train_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "valid_iterator = DataLoader(valid_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "test_iterator = DataLoader(test_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "\n",
    "print(\"Train batch:\", len(train_iterator))\n",
    "print(\"Valid batch:\", len(valid_iterator))\n",
    "print(\"Test batch:\", len(test_iterator))\n",
    "\n",
    "input_dims = [80,36,300,20]\n",
    "\n",
    "runs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeJ9bXYeTieb"
   },
   "source": [
    "### Text classifier mdoel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iko1tq0fWoMt"
   },
   "outputs": [],
   "source": [
    "######################### Test classifier model\n",
    "text_in = input_dims[2]\n",
    "text_seq = input_dims[3]\n",
    "\n",
    "d_qkv = text_in//n_head\n",
    "d_model = text_in\n",
    "d_inner = text_in\n",
    "\n",
    "model = TextSubNet_attention(\n",
    "      n_src_vocab=input_dims[3], d_word_vec=input_dims[2], n_layers=n_layers, n_head=n_head, d_q=d_qkv, d_k=d_qkv, d_v=d_qkv,\n",
    "      d_model=d_model, d_inner=d_inner, pad_idx=None, dropout=tdr, n_position=text_seq, scale_emb=False,\n",
    "      emb_shape = [[16,16,8,5],[4,4,8,4]],emb_rank = 16, emb_tensor_type = 'TensorTrainMatrix',\n",
    "      attention_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], attention_rank = [max_rank,max_rank],attention_tensor_type = 'TensorTrainMatrix',\n",
    "      ffn_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], ffn_rank = [max_rank,max_rank],ffn_tensor_type = 'TensorTrainMatrix',\n",
    "      d_classifier=d_inner, num_class = output_dim, dropout_classifier = 0.2,\n",
    "      classifier_shape = [[5,4,3,5],[1,2,1,1]],classifier_rank = max_rank,classifier_tensor_type = 'TensorTrainMatrix',\n",
    "      bit_attn = 8, scale_attn = 2**(-5),\n",
    "      bit_ffn = 8, scale_ffn = 2**(-5),\n",
    "      bit_a = 8, scale_a = 2**(-5),\n",
    "      quantized = False,\n",
    "      tensorized=True)\n",
    "\n",
    "if cuda == 1:\n",
    "    model = model.cuda()\n",
    "    DTYPE = torch.cuda.FloatTensor\n",
    "    LONG = torch.cuda.LongTensor\n",
    "\n",
    "x = next(iter(train_iterator))\n",
    "x_a = torch.cat((Variable(x[0].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_sz,(input_dims[0]-data_dims[0])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "x_v = torch.cat((Variable(x[1].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_sz,(input_dims[1]-data_dims[1])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "y = Variable(x[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "src_mask = None\n",
    "output   = model(x_t, src_mask, return_attns=False)\n",
    "\n",
    "loss = criterion(output, y)\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "  print(name, layer)\n",
    "\n",
    "'''\n",
    "print(x_a.size())\n",
    "print(x_v.size())\n",
    "print(x_t.size())\n",
    "\n",
    "print(x_a)\n",
    "print(x_v)\n",
    "print(x_t)\n",
    "print(y)\n",
    "print(output)\n",
    "print(loss)\n",
    "\n",
    "output = output.cpu().data.numpy().reshape(-1, output_dim)\n",
    "y = y.cpu().data.numpy().reshape(-1, output_dim)\n",
    "\n",
    "all_true_label = np.argmax(y,axis=1)\n",
    "all_predicted_label = np.argmax(output,axis=1)\n",
    "\n",
    "print(all_true_label)\n",
    "print(all_predicted_label)\n",
    "\n",
    "f1 = f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "acc_score = accuracy_score(all_true_label, all_predicted_label)\n",
    "\n",
    "print(f1)\n",
    "print(acc_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UrD-7pdYkm1"
   },
   "source": [
    "## BP fusion model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKJRnvRDLXar"
   },
   "outputs": [],
   "source": [
    "# for max_rank in rank_list:\n",
    "'''\n",
    "for fusion_rank in fusion_rank_list:\n",
    "  for max_rank in max_rank_list:\n",
    "    for run in range(runs):\n",
    "        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,fusion_rank,max_rank,run))\n",
    "'''\n",
    "'''\n",
    "for max_rank in max_rank_list:\n",
    "  for fusion_rank in fusion_rank_list:\n",
    "    for n_layers in n_layers_list:\n",
    "      for n_head in n_head_list:\n",
    "'''\n",
    "'''\n",
    "for emotion in emotion_list:\n",
    "  train_set, valid_set, test_set, input_dims = load_iemocap(data_path, emotion)\n",
    "  train_iterator = DataLoader(train_set, batch_size=batch_sz, num_workers=2, shuffle=True)\n",
    "  valid_iterator = DataLoader(valid_set, batch_size=len(valid_set), num_workers=2, shuffle=True)\n",
    "  test_iterator = DataLoader(test_set, batch_size=len(test_set), num_workers=2, shuffle=True)\n",
    "\n",
    "'''\n",
    "'''\n",
    "for lr in lr_list:\n",
    "  for decay in decay_list:\n",
    "    for run in range(runs):\n",
    "      model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,lr,decay,run))\n",
    "'''\n",
    "'''\n",
    "for TT_FUSION in TT_FUSION_list:\n",
    "  if TT_FUSION == 0:\n",
    "    signiture = 'attn_0'\n",
    "    lr = 0.0005\n",
    "    decay = 0.01\n",
    "  else:\n",
    "    signiture = 'attn_1'\n",
    "    lr = 0.003\n",
    "    decay = 0.002\n",
    "  for max_rank in max_rank_list:\n",
    "    for fusion_rank in fusion_rank_list:\n",
    "      for run in range(runs):\n",
    "        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "'''\n",
    "for use_LSTM in use_LSTM_list:\n",
    "  if use_LSTM == 0:\n",
    "    signiture = 'LSTM_0'\n",
    "  else:\n",
    "    signiture = 'ATTN_0'\n",
    "  for max_rank in max_rank_list:\n",
    "    for fusion_rank in fusion_rank_list:\n",
    "      for run in range(runs):\n",
    "        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "        print(\"Temp location for models: {}\".format(model_path))\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        best_valid_acc = 0\n",
    "        best_valid_loss = float('inf')\n",
    "        ######################### Fusion Model\n",
    "        model = LMF(input_dims, (ahid, vhid, thid), audio_out, video_out, text_out, n_layers, n_head, dropouts, output_dim, fusion_rank, max_rank, TT_FUSION, use_LSTM, 'TTM',device, dtype)\n",
    "\n",
    "        ######################### Test classifier model\n",
    "        text_in = input_dims[2]\n",
    "        text_seq = input_dims[3]\n",
    "\n",
    "        d_qkv = text_in//n_head\n",
    "        d_model = text_in\n",
    "        d_inner = text_in\n",
    "\n",
    "        text_prob = dropouts[2]\n",
    "        model = TextSubNet_attention(\n",
    "              n_src_vocab=input_dims[3], d_word_vec=input_dims[2], n_layers=n_layers, n_head=n_head, d_q=d_qkv, d_k=d_qkv, d_v=d_qkv,\n",
    "              d_model=d_model, d_inner=d_inner, pad_idx=None, dropout=text_prob, n_position=text_seq, scale_emb=False,\n",
    "              emb_shape = [[16,16,8,5],[4,4,8,4]],emb_rank = 16, emb_tensor_type = 'TensorTrainMatrix',\n",
    "              attention_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], attention_rank = [max_rank,max_rank],attention_tensor_type = 'TensorTrainMatrix',\n",
    "              ffn_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], ffn_rank = [max_rank,max_rank],ffn_tensor_type = 'TensorTrainMatrix',\n",
    "              d_classifier=d_inner, num_class = output_dim, dropout_classifier = 0.2,\n",
    "              classifier_shape = [[5,4,3,5],[5,4,3,5]],classifier_rank = max_rank,classifier_tensor_type = 'TensorTrainMatrix',\n",
    "              bit_attn = 8, scale_attn = 2**(-5),\n",
    "              bit_ffn = 8, scale_ffn = 2**(-5),\n",
    "              bit_a = 8, scale_a = 2**(-5),\n",
    "              quantized = False,\n",
    "              tensorized=True)\n",
    "\n",
    "        ######################### Continue\n",
    "        if cuda == 1:\n",
    "            model = model.cuda()\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "            LONG = torch.cuda.LongTensor\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        # factors = list(model.parameters())[:3]\n",
    "        # other = list(model.parameters())[3:]\n",
    "        # optimizer = optim.Adam([{\"params\": factors, \"lr\": factor_lr}, {\"params\": other, \"lr\": lr}], weight_decay=decay)\n",
    "\n",
    "        # criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        optimizer = optim.Adam(list(model.parameters()), lr, weight_decay=decay)\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)    # 3\n",
    "\n",
    "        comment = f'rank = {max_rank} run = {run}'\n",
    "        writer = SummaryWriter(comment=comment, log_dir='./tt_runs')\n",
    "\n",
    "        for e in range(epochs):\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            avg_train_loss = 0.0\n",
    "            for batch in train_iterator:\n",
    "                model.zero_grad()\n",
    "\n",
    "                x = batch[:-1]\n",
    "                x_a = Variable(x[0].float().type(DTYPE), requires_grad=False)\n",
    "                x_v = Variable(x[1].float().type(DTYPE), requires_grad=False)\n",
    "                x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "                y = Variable(batch[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "                try:\n",
    "                    output = model(x_a, x_v, x_t)\n",
    "                except ValueError as e:\n",
    "                    print(x_a.data.shape)\n",
    "                    print(x_v.data.shape)\n",
    "                    print(x_t.data.shape)\n",
    "                    raise e\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                avg_train_loss += loss.data.item()\n",
    "                optimizer.step()\n",
    "\n",
    "            avg_train_loss = avg_train_loss / len(train_set)\n",
    "            print(\"Epoch {} complete! Average Training loss: {}\".format(e, avg_train_loss))\n",
    "            writer.add_scalar(\"Avg Train Loss\", avg_train_loss, e)\n",
    "\n",
    "            # Terminate the training process if run into NaN\n",
    "            if np.isnan(avg_train_loss):\n",
    "                print(\"Training got into NaN values...\\n\\n\")\n",
    "                complete = False\n",
    "                break\n",
    "\n",
    "            model.eval()\n",
    "            avg_valid_loss = 0.0\n",
    "            for batch in valid_iterator:\n",
    "                x = batch[:-1]\n",
    "                x_a = Variable(x[0].float().type(DTYPE), requires_grad=False)\n",
    "                x_v = Variable(x[1].float().type(DTYPE), requires_grad=False)\n",
    "                x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "                y = Variable(batch[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "                output = model(x_a, x_v, x_t)\n",
    "                valid_loss = criterion(output, y)\n",
    "                avg_valid_loss += valid_loss.data.item()\n",
    "\n",
    "            if np.isnan(avg_valid_loss):\n",
    "                print(\"Training got into NaN values...\\n\\n\")\n",
    "                complete = False\n",
    "                break\n",
    "\n",
    "            avg_valid_loss = avg_valid_loss / len(valid_set)\n",
    "            print(\"Validation loss is: {}\".format(avg_valid_loss))\n",
    "            writer.add_scalar(\"Avg Valid Loss\", avg_valid_loss, e)\n",
    "\n",
    "            all_true_label = np.argmax( y.cpu().data.numpy().reshape(-1, output_dim),axis=1)\n",
    "            all_predicted_label = np.argmax(output.cpu().data.numpy().reshape(-1, output_dim),axis=1)\n",
    "\n",
    "            acc_score = accuracy_score(all_true_label, all_predicted_label)\n",
    "\n",
    "\n",
    "            if (avg_valid_loss < best_valid_loss):\n",
    "                curr_patience = patience\n",
    "                best_valid_loss = avg_valid_loss\n",
    "                torch.save(model, model_path)\n",
    "                print(\"Found new best model, saving to disk...\")\n",
    "            else:\n",
    "                curr_patience -= 1\n",
    "\n",
    "            if curr_patience <= 0:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "#         ------------Code to Write Best Metrics(F1/Acc) out of Run --------------\n",
    "#         best_model = torch.load(model_path)\n",
    "#         best_model.eval()\n",
    "#         best_model.to(device)\n",
    "\n",
    "#         for batch in test_iterator:\n",
    "#             x = batch[:-1]\n",
    "#             x_a = Variable(x[0].float().type(DTYPE), requires_grad=False)\n",
    "#             x_v = Variable(x[1].float().type(DTYPE), requires_grad=False)\n",
    "#             x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "#             y = Variable(batch[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "#             output_test = best_model(x_a, x_v, x_t)\n",
    "\n",
    "#         output_test = output_test.cpu().data.numpy().reshape(-1, output_dim)\n",
    "#         y = y.cpu().data.numpy().reshape(-1, output_dim)\n",
    "\n",
    "#         # these are the needed metrics\n",
    "#         all_true_label = np.argmax(y,axis=1)\n",
    "#         all_predicted_label = np.argmax(output_test,axis=1)\n",
    "\n",
    "#         f1 = f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "#         acc_score = accuracy_score(all_true_label, all_predicted_label)\n",
    "\n",
    "\n",
    "#         writer.add_hparams(\n",
    "#             {\"max_rank\": max_rank, \"run\": run},\n",
    "#             {\n",
    "#                 'hparam/accuracy': acc_score, 'hparam/f1': f1\n",
    "#             }\n",
    "#         )\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkuRLICBLXat"
   },
   "source": [
    "## Eval Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRyJNmLZLXat"
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.cuda.FloatTensor\n",
    "def eval_at_model_path(model_path, test_iterator):\n",
    "    best_model = torch.load(model_path)\n",
    "    best_model.eval()\n",
    "    best_model.to(device)\n",
    "\n",
    "    for batch in test_iterator:\n",
    "        x = batch[:-1]\n",
    "        x_a = Variable(x[0].float().type(DTYPE), requires_grad=False)\n",
    "        x_v = Variable(x[1].float().type(DTYPE), requires_grad=False)\n",
    "        x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "        y = Variable(batch[-1].view(-1, output_dim).float().type(torch.cuda.LongTensor), requires_grad=False)\n",
    "        # old\n",
    "        output_test = best_model(x_a, x_v, x_t)\n",
    "        # new\n",
    "        # X = [x_a, x_v, x_t]\n",
    "        # output_test = best_model(X)\n",
    "\n",
    "\n",
    "    output_test = output_test.cpu().data.numpy().reshape(-1, output_dim)\n",
    "    y = y.cpu().data.numpy().reshape(-1, output_dim)\n",
    "\n",
    "\n",
    "    # these are the needed metrics\n",
    "    all_true_label = np.argmax(y,axis=1)\n",
    "    all_predicted_label = np.argmax(output_test,axis=1)\n",
    "\n",
    "    f1 = f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "    acc_score = accuracy_score(all_true_label, all_predicted_label)\n",
    "\n",
    "    display(f1, acc_score)\n",
    "\n",
    "    return acc_score\n",
    "    # print(\"LSTM HH Max Rank\", model.text_subnet.rnn.layer_hh.weight_tensor.estimate_rank(), model.text_subnet.rnn.layer_hh.weight_tensor.tensor_type)\n",
    "    # print(\"LSTM IH Max Rank\", model.text_subnet.rnn.layer_ih.weight_tensor.estimate_rank(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r79jMnY-YdxX"
   },
   "source": [
    "## ZO-text attn training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMcMgcxLuBtz"
   },
   "outputs": [],
   "source": [
    "# ZO_SGD_estimate\n",
    "import math\n",
    "import copy\n",
    "pi = torch.tensor(math.pi)\n",
    "\n",
    "def perturb_bernoulli(weight, sigma):\n",
    "  mask = 0.5*torch.ones_like(weight)\n",
    "  mask = -sigma*torch.ones_like(weight) + 2*sigma*torch.bernoulli(mask)\n",
    "\n",
    "  return mask\n",
    "\n",
    "def perturb_gaussian(weight, sigma):\n",
    "  w_size = weight.size()\n",
    "\n",
    "  return torch.normal(0, sigma, size=w_size)\n",
    "\n",
    "def ZO_SGD_estimate(model, criterion, data, target, sigma, K, lr, signSGD=True, sparse=True, symmetric=True) -> list:\n",
    "  pred = model(data)\n",
    "  loss = criterion(pred, target)\n",
    "  # estimate gradient\n",
    "  G = []\n",
    "  for k in range(K):\n",
    "    ##################### f(x+delta_k) start ##########################\n",
    "    g_model=copy.deepcopy(model)\n",
    "    for m in g_model.modules():\n",
    "      if isinstance(m, nn.Linear):\n",
    "        # Tensorized\n",
    "        if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "          # TT-Matrix\n",
    "          if isinstance(m.tensor, TensorTrainMatrix):\n",
    "            dims = len(m.tensor.dims[0])\n",
    "            if sparse == True:\n",
    "              dim_range = [dims-1]\n",
    "            else:\n",
    "              dim_range = range(dims)\n",
    "            # dims = 1\n",
    "            m.delta_ = []\n",
    "            # print(dims)\n",
    "            # print('not perturbed', m.tensor.factors[0].data[0,0,0,0])\n",
    "            for dim in dim_range:\n",
    "              tt_core = m.tensor.factors[dim].data\n",
    "              delta_ = perturb_gaussian(tt_core, sigma)\n",
    "              delta_ = delta_.to(device)\n",
    "              m.delta_.append(delta_)\n",
    "\n",
    "              tt_core = tt_core + delta_\n",
    "              m.tensor.factors[dim] = nn.Parameter(tt_core)\n",
    "\n",
    "            # print('perturbed', m.tensor.factors[0].data[0,0,0,0])\n",
    "\n",
    "        # not tensorized\n",
    "        else:\n",
    "          weight = m.weight.data\n",
    "          # print('model', weight[0,0])\n",
    "          m.delta_ = perturb_gaussian(weight, sigma)\n",
    "          m.delta_ = m.delta_.to(device)\n",
    "          m.weight = nn.Parameter(weight + m.delta_)\n",
    "          # print('x+delta_k',m.weight.data[0,0])\n",
    "\n",
    "    pred_1 = g_model(data)\n",
    "    loss_1 = criterion(pred_1, target)\n",
    "    ##################### f(x+delta_k) end ##########################\n",
    "    if symmetric == True:\n",
    "      ##################### f(x-delta_k) start ##########################\n",
    "      # use the same g_model to copy its m.delta_\n",
    "      for m in g_model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "          # Tensorized\n",
    "          if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "            # TT-Matrix\n",
    "            if isinstance(m.tensor, TensorTrainMatrix):\n",
    "              dims = len(m.tensor.dims[0])\n",
    "              # dims = 1\n",
    "              # print(dims)\n",
    "              # print('not perturbed', m.tensor.factors[0].data[0,0,0,0])\n",
    "              if sparse == True:\n",
    "                dim_range = [dims-1]\n",
    "              else:\n",
    "                dim_range = range(dims)\n",
    "              for dim in dim_range:\n",
    "                tt_core = m.tensor.factors[dim].data\n",
    "                # delta_ = m.delta_[dim]\n",
    "                delta_ = m.delta_.pop(0)\n",
    "\n",
    "                tt_core = tt_core - 2 * delta_\n",
    "                m.tensor.factors[dim] = nn.Parameter(tt_core)\n",
    "                m.delta_.append(delta_)\n",
    "              # print('perturbed', m.tensor.factors[0].data[0,0,0,0])\n",
    "\n",
    "          # not tensorized\n",
    "          else:\n",
    "            weight = m.weight.data\n",
    "            # print('model',weight[0,0])\n",
    "            m.weight = nn.Parameter(weight - 2 * m.delta_)\n",
    "            # print('x-delta_k',m.weight.data[0,0])\n",
    "\n",
    "      pred_2 = g_model(data)\n",
    "      loss_2 = criterion(pred_2, target)\n",
    "\n",
    "      # accumulate gradients\n",
    "      for m in g_model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "          # Tensorized\n",
    "          if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "            # TT-Matrix\n",
    "            if isinstance(m.tensor, TensorTrainMatrix):\n",
    "              dims = len(m.tensor.dims[0])\n",
    "              if sparse == True:\n",
    "                dim_range = [dims-1]\n",
    "              else:\n",
    "                dim_range = range(dims)\n",
    "              # dims = 1\n",
    "              if k == 0:\n",
    "                g_ = []\n",
    "                for dim in dim_range:\n",
    "                  # delta_ = m.delta_[dim]\n",
    "                  delta_ = m.delta_.pop(0)\n",
    "                  g = (loss_1-loss_2)/(2*sigma**2)/K * delta_\n",
    "                  g_.append(g)\n",
    "                  m.delta_.append(delta_)\n",
    "\n",
    "                G.append(g_)\n",
    "              else:\n",
    "                g_ = G.pop(0)\n",
    "                for dim in dim_range:\n",
    "                  # delta_ = m.delta_[dim]\n",
    "                  delta_ = m.delta_.pop(0)\n",
    "                  g = g_.pop(0)\n",
    "                  g = g + (loss_1-loss_2)/(2*sigma**2)/K * delta_\n",
    "                  g_.append(g)\n",
    "                  m.delta_.append(delta_)\n",
    "\n",
    "                G.append(g_)\n",
    "              # print('g_=',g_[0][0,0,0,0])\n",
    "          # Not Tensorized\n",
    "          else:\n",
    "            delta_ = m.delta_\n",
    "            if k == 0:\n",
    "              g = (loss_1-loss_2)/(2*sigma**2)/K * delta_\n",
    "              G.append(g)\n",
    "            else:\n",
    "              g = G.pop(0)\n",
    "              g = g + (loss_1-loss_2)/(2*sigma**2)/K * delta_\n",
    "              G.append(g)\n",
    "\n",
    "      ##################### f(x-delta_k) end ##########################\n",
    "    else:\n",
    "      ##################### f(x) ##########################\n",
    "      for m in g_model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "          # Tensorized\n",
    "          if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "            # TT-Matrix\n",
    "            if isinstance(m.tensor, TensorTrainMatrix):\n",
    "              dims = len(m.tensor.dims[0])\n",
    "              if sparse == True:\n",
    "                dim_range = [dims-1]\n",
    "              else:\n",
    "                dim_range = range(dims)\n",
    "              # dims = 1\n",
    "              if k == 0:\n",
    "                g_ = []\n",
    "                for dim in dim_range:\n",
    "                  # delta_ = m.delta_[dim]\n",
    "                  delta_ = m.delta_.pop(0)\n",
    "                  g = (loss_1-loss)/(sigma**2)/K * delta_\n",
    "                  g_.append(g)\n",
    "                  m.delta_.append(delta_)\n",
    "\n",
    "                G.append(g_)\n",
    "              else:\n",
    "                g_ = G.pop(0)\n",
    "                for dim in dim_range:\n",
    "                  # delta_ = m.delta_[dim]\n",
    "                  delta_ = m.delta_.pop(0)\n",
    "                  g = g_.pop(0)\n",
    "                  g = g + (loss_1-loss)/(sigma**2)/K * delta_\n",
    "                  g_.append(g)\n",
    "                  m.delta_.append(delta_)\n",
    "\n",
    "                G.append(g_)\n",
    "              # print('g_=',g_[0][0,0,0,0])\n",
    "          # Not Tensorized\n",
    "          else:\n",
    "            delta_ = m.delta_\n",
    "            if k == 0:\n",
    "              g = (loss_1-loss)/(sigma**2)/K * delta_\n",
    "              G.append(g)\n",
    "            else:\n",
    "              g = G.pop(0)\n",
    "              g = g + (loss_1-loss)/(sigma**2)/K * delta_\n",
    "              G.append(g)\n",
    "  # estimate grad end\n",
    "  ############################### SGD ################################\n",
    "  for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "      # tensorized\n",
    "      if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "        if isinstance(m.tensor, TensorTrainMatrix):\n",
    "          dims = len(m.tensor.dims[0])\n",
    "          if sparse == True:\n",
    "            dim_range = [dims-1]\n",
    "          else:\n",
    "            dim_range = range(dims)\n",
    "          # dims = 1\n",
    "          g_ = G.pop(0)\n",
    "          # print('SGD g_=',g_[0][0,0,0,0])\n",
    "          for dim in dim_range:\n",
    "            g = g_.pop(0)\n",
    "            if signSGD == True:\n",
    "              # print('tt-core', m.tensor.factors[dim])\n",
    "              # print('grad', g)\n",
    "              m.tensor.factors[dim] = nn.Parameter(m.tensor.factors[dim] - lr*torch.sign(g))\n",
    "            else:\n",
    "              m.tensor.factors[dim] = nn.Parameter(m.tensor.factors[dim] - lr*g)\n",
    "\n",
    "          # print('updated', m.tensor.factors[0].data[0,0,0,0])\n",
    "      # not tensorized\n",
    "      else:\n",
    "        g = G.pop(0)\n",
    "        m.weight = nn.Parameter(m.weight - lr*g)\n",
    "  return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jh3c25DOHzh5"
   },
   "outputs": [],
   "source": [
    "##################### ZO_SCD_estimate ##########################\n",
    "def ZO_SCD_estimate(model, criterion, data, target, lr, sparse_a=1, sparse_s=0.05, symmetric=True):\n",
    "  old_pred = model(data)\n",
    "  old_loss = criterion(old_pred, target)\n",
    "\n",
    "  for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "      # Tensorized\n",
    "      if isinstance(m, (TensorizedLinear, TensorizedLinear_module)):\n",
    "        # TT-Matrix\n",
    "        if isinstance(m.tensor, TensorTrainMatrix):\n",
    "          dims = len(m.tensor.dims[0])\n",
    "          dim_range = range(min(dims,sparse_a))\n",
    "          # dim_range = [0]\n",
    "          # print(dims)\n",
    "          # print('not perturbed', m.tensor.factors[0].data[0,0,0,0])\n",
    "          for dim in dim_range:\n",
    "            tt_shape = m.tensor.factors[dim].size()\n",
    "            tt_core = m.tensor.factors[dim].view(-1)\n",
    "\n",
    "            mask = sparse_s*torch.ones_like(tt_core)\n",
    "            mask = torch.bernoulli(mask)\n",
    "\n",
    "            for idx in range(torch.numel(tt_core)):\n",
    "              if mask[idx] == 1:\n",
    "                old_value = tt_core[idx]\n",
    "                pos_perturbed_value = old_value + lr\n",
    "                neg_perturbed_value = old_value - lr\n",
    "\n",
    "                tt_core[idx] = pos_perturbed_value\n",
    "                m.tensor.factors[dim] = tt_core.reshape(tt_shape)\n",
    "\n",
    "                new_pred = model(data)\n",
    "                new_loss = criterion(new_pred, target)\n",
    "\n",
    "                if new_loss < old_loss:\n",
    "                  old_loss = new_loss\n",
    "                else:\n",
    "                  tt_core[idx] = neg_perturbed_value\n",
    "                  m.tensor.factors[dim] = tt_core.reshape(tt_shape)\n",
    "                  # old_pred = model(data)\n",
    "                  # old_loss = criterion(old_pred, target)\n",
    "                  new_pred = model(data)\n",
    "                  new_loss = criterion(new_pred, target)\n",
    "\n",
    "                  if new_loss < old_loss:\n",
    "                    old_loss = new_loss\n",
    "                  else:\n",
    "                    tt_core[idx] = old_value\n",
    "                    m.tensor.factors[dim] = tt_core.reshape(tt_shape)\n",
    "      # not tensorized\n",
    "      else:\n",
    "        weight = m.weight.data\n",
    "        # print('model', weight[0,0])\n",
    "        m.delta_ = perturb_gaussian(weight, sigma)\n",
    "        m.delta_ = m.delta_.to(device)\n",
    "        m.weight = nn.Parameter(weight + m.delta_)\n",
    "        # print('x+delta_k',m.weight.data[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP-WBBu3TuCf"
   },
   "source": [
    "## ZO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UW-3CcpYcuO"
   },
   "outputs": [],
   "source": [
    "signiture = 'BP_text'\n",
    "run = 1\n",
    "\n",
    "SGD_lr = 0.002\n",
    "SGD_gamma = 0.985\n",
    "sigma = 0.1\n",
    "n_sample = 10\n",
    "K = 10\n",
    "\n",
    "SCD_lr = 0.0002\n",
    "SCD_gamma = 0.985\n",
    "my_sparse_a = 3\n",
    "my_sparse_s = 0.1\n",
    "\n",
    "my_sparse_a = 3\n",
    "my_sparse_s = 0.1\n",
    "\n",
    "\n",
    "model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,lr, decay, run))\n",
    "print(\"Temp location for models: {}\".format(model_path))\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "best_valid_acc = 0\n",
    "best_valid_loss = float('inf')\n",
    "######################### Fusion Model\n",
    "# model = LMF(input_dims, (ahid, vhid, thid), audio_out, video_out, text_out, n_layers, n_head, dropouts, output_dim, fusion_rank, max_rank, TT_FUSION, use_LSTM, 'TTM',device, dtype)\n",
    "\n",
    "\n",
    "audio_in = input_dims[0]\n",
    "video_in = input_dims[1]\n",
    "text_in = input_dims[2]\n",
    "text_seq = input_dims[3]\n",
    "\n",
    "d_qkv = text_in//n_head\n",
    "d_model = text_in\n",
    "d_inner = text_in\n",
    "\n",
    "######################### Audio classifier model\n",
    "'''\n",
    "model = SubNet(audio_in, ahid, output_dim, adr,\n",
    "              shape=[[[2,4,2,5],[4,2,2,2]], [[2,2,2,4],[4,2,2,2]], [[2,2,2,4],[1,2,1,1]]],\n",
    "              max_rank=max_rank, tensor_type='TensorTrainMatrix', device=device, dtype=dtype)\n",
    "'''\n",
    "######################### Video classifier model\n",
    "'''\n",
    "model = SubNet(video_in, vhid, output_dim, vdr,\n",
    "              shape=[[[3,2,2,3],[4,2,2,2]], [[2,2,2,4],[4,2,2,2]], [[2,2,2,4],[1,2,1,1]]],\n",
    "              max_rank=max_rank, tensor_type=tensor_type, device=device, dtype=dtype)\n",
    "'''\n",
    "\n",
    "######################### Test classifier model\n",
    "\n",
    "model = TextSubNet_attention(\n",
    "      n_src_vocab=input_dims[3], d_word_vec=input_dims[2], n_layers=n_layers, n_head=n_head, d_q=d_qkv, d_k=d_qkv, d_v=d_qkv,\n",
    "      d_model=d_model, d_inner=d_inner, pad_idx=None, dropout=tdr, n_position=text_seq, scale_emb=False,\n",
    "      emb_shape = [[16,16,8,5],[4,4,8,4]],emb_rank = 16, emb_tensor_type = 'TensorTrainMatrix',\n",
    "      attention_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], attention_rank = [max_rank,max_rank],attention_tensor_type = 'TensorTrainMatrix',\n",
    "      ffn_shape = [[[5,4,3,5],[5,4,3,5]],[[5,4,3,5],[5,4,3,5]]], ffn_rank = [max_rank,max_rank],ffn_tensor_type = 'TensorTrainMatrix',\n",
    "      d_classifier=d_inner, num_class = output_dim, dropout_classifier = 0.2,\n",
    "      classifier_shape = [[5,4,3,5],[1,2,1,1]],classifier_rank = max_rank,classifier_tensor_type = 'TensorTrainMatrix',\n",
    "      bit_attn = 8, scale_attn = 2**(-5),\n",
    "      bit_ffn = 8, scale_ffn = 2**(-5),\n",
    "      bit_a = 8, scale_a = 2**(-5),\n",
    "      quantized = False,\n",
    "      tensorized=True)\n",
    "\n",
    "\n",
    "######################### Continue\n",
    "if cuda == 1:\n",
    "    model = model.cuda()\n",
    "    DTYPE = torch.cuda.FloatTensor\n",
    "    LONG = torch.cuda.LongTensor\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "# factors = list(model.parameters())[:3]\n",
    "# other = list(model.parameters())[3:]\n",
    "# optimizer = optim.Adam([{\"params\": factors, \"lr\": factor_lr}, {\"params\": other, \"lr\": lr}], weight_decay=decay)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.Adam(list(model.parameters()), lr, weight_decay=decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)    # 3\n",
    "\n",
    "comment = f'rank = {max_rank} run = {run}'\n",
    "writer = SummaryWriter(comment=comment, log_dir='./tt_runs')\n",
    "\n",
    "train_len = len(train_iterator.dataset)\n",
    "train_batch = len(train_iterator)\n",
    "\n",
    "valid_len = len(valid_iterator.dataset)\n",
    "valid_batch = len(valid_iterator)\n",
    "\n",
    "for e in range(epochs):\n",
    "    ########################## training ############################\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    avg_train_loss = 0.0\n",
    "    batch_cnt = 0\n",
    "    for batch in train_iterator:\n",
    "        batch_cnt += 1\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = batch[:-1]\n",
    "        batch_rows = x[0].data.shape[0]\n",
    "        x_a = torch.cat((Variable(x[0].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_rows,(input_dims[0]-data_dims[0])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "        x_v = torch.cat((Variable(x[1].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_rows,(input_dims[1]-data_dims[1])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "        x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "        y = Variable(batch[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "        ########################## BP ############################\n",
    "        '''\n",
    "        # output = model(x_a)\n",
    "        # output = model(x_v)\n",
    "        output = model(x_t)\n",
    "        # X = [x_a, x_v, x_t]\n",
    "        # output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        avg_train_loss += loss.data.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "\n",
    "        ########################## ZO ############################\n",
    "        with torch.no_grad():\n",
    "          # output = model(x_a)\n",
    "          # output = model(x_v)\n",
    "          output = model(x_t)\n",
    "          # X = [x_a, x_v, x_t]\n",
    "          # output = model(X)\n",
    "          loss = criterion(output, y)\n",
    "\n",
    "          if e < 15:\n",
    "            ZO_SGD_estimate(model, criterion, x_t, y, sigma, K, SGD_lr, signSGD=True, sparse=False, symmetric=True)\n",
    "          else:\n",
    "            ZO_SCD_estimate(model, criterion, x_t, y, lr, sparse_a=my_sparse_a, sparse_s=my_sparse_s, symmetric=True)\n",
    "            # ZO_SGD_estimate(model, criterion, x_t, y, sigma, K, lr, signSGD=False, sparse=True, symmetric=True)\n",
    "\n",
    "        avg_train_loss += loss.data.item()\n",
    "\n",
    "        if batch_cnt % 10 == 0:\n",
    "          # print(\"epoch {} batch {} SGD Train loss is: {}\".format(e, batch_cnt, loss.data.item()))\n",
    "          writer.add_scalar(\"SGD Train loss\", loss.data.item(), batch_cnt)\n",
    "\n",
    "    if e % 10 == 9:\n",
    "        SGD_lr = SGD_lr * SGD_gamma\n",
    "\n",
    "    avg_train_loss = avg_train_loss / train_batch\n",
    "    print(\"\\n Epoch {} complete! Average Training loss: {}\".format(e, avg_train_loss))\n",
    "    writer.add_scalar(\"Avg Train Loss\", avg_train_loss, e)\n",
    "\n",
    "    # Terminate the training process if run into NaN\n",
    "    if np.isnan(avg_train_loss):\n",
    "        print(\"Training got into NaN values...\\n\\n\")\n",
    "        complete = False\n",
    "        break\n",
    "\n",
    "    ########################## Eval ############################\n",
    "    model.eval()\n",
    "    avg_valid_loss = 0.0\n",
    "    f1 = 0.0\n",
    "    acc_score = 0.0\n",
    "    for batch in valid_iterator:\n",
    "        x = batch[:-1]\n",
    "        batch_rows = x[0].data.shape[0]\n",
    "        x_a = torch.cat((Variable(x[0].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_rows,(input_dims[0]-data_dims[0])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "        x_v = torch.cat((Variable(x[1].float().type(DTYPE), requires_grad=False), Variable(torch.zeros(batch_rows,(input_dims[1]-data_dims[1])).type(DTYPE), requires_grad=False)), dim=1)\n",
    "        x_t = Variable(x[2].float().type(DTYPE), requires_grad=False)\n",
    "        y = Variable(batch[-1].view(-1, output_dim).float().type(DTYPE), requires_grad=False)\n",
    "\n",
    "        src_mask = None\n",
    "        # output = model(x_a)\n",
    "        # output = model(x_v)\n",
    "        output = model(x_t)\n",
    "        # X = [x_a, x_v, x_t]\n",
    "        # output = model(X)\n",
    "\n",
    "        valid_loss = criterion(output, y)\n",
    "        avg_valid_loss += valid_loss.data.item()\n",
    "\n",
    "        output = output.cpu().data.numpy().reshape(-1, output_dim)\n",
    "        y = y.cpu().data.numpy().reshape(-1, output_dim)\n",
    "\n",
    "        # metrics\n",
    "        all_true_label = np.argmax(y,axis=1)\n",
    "        all_predicted_label = np.argmax(output,axis=1)\n",
    "\n",
    "        f1 += f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "        acc_score += accuracy_score(all_true_label, all_predicted_label)\n",
    "\n",
    "    if np.isnan(avg_valid_loss):\n",
    "        print(\"Training got into NaN values...\\n\\n\")\n",
    "        complete = False\n",
    "        break\n",
    "\n",
    "    # show avg_valid_loss， acc_score and f1\n",
    "    avg_valid_loss = avg_valid_loss / valid_batch\n",
    "    print(\"Validation loss is: {}\".format(avg_valid_loss))\n",
    "    writer.add_scalar(\"Avg Valid Loss\", avg_valid_loss, e)\n",
    "\n",
    "    f1 = f1 / valid_batch\n",
    "    acc_score = acc_score / valid_batch\n",
    "    print(\"F1-score on test set is {}\".format(f1))\n",
    "    print(\"Accuracy score on test set is {}\".format(acc_score))\n",
    "\n",
    "\n",
    "    if (avg_valid_loss < best_valid_loss):\n",
    "        curr_patience = patience\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        torch.save(model, model_path)\n",
    "        print(\"Found new best model, saving to disk...\")\n",
    "    else:\n",
    "        curr_patience -= 1\n",
    "\n",
    "    '''\n",
    "    if curr_patience <= 0:\n",
    "        break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZO Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28426,
     "status": "ok",
     "timestamp": 1687508504398,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": -480
    },
    "id": "pc1f5LMXLXau",
    "outputId": "982cf354-1755-47a5-ed91-2fd66f397c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n",
      "../history/tt_fusion_layer_head/model_lmf_happy_2_4_1_2_0.pt\n",
      "F1-score on test set is 0.8223518181720171\n",
      "Accuracy score on test set is 0.8635394456289979\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_happy_2_4_1_2_1.pt\n",
      "F1-score on test set is 0.8339447188743941\n",
      "Accuracy score on test set is 0.8678038379530917\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_happy_2_4_1_2_2.pt\n",
      "F1-score on test set is 0.7896951608577283\n",
      "Accuracy score on test set is 0.8560767590618337\n",
      "-------------\n",
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n",
      "../history/tt_fusion_layer_head/model_lmf_sad_2_4_1_2_0.pt\n",
      "F1-score on test set is 0.8275418051577313\n",
      "Accuracy score on test set is 0.8304904051172708\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_sad_2_4_1_2_1.pt\n",
      "F1-score on test set is 0.816002443068629\n",
      "Accuracy score on test set is 0.8219616204690832\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_sad_2_4_1_2_2.pt\n",
      "F1-score on test set is 0.818230472799828\n",
      "Accuracy score on test set is 0.8251599147121536\n",
      "-------------\n",
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n",
      "../history/tt_fusion_layer_head/model_lmf_angry_2_4_1_2_0.pt\n",
      "F1-score on test set is 0.8280271120450827\n",
      "Accuracy score on test set is 0.8198294243070362\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_angry_2_4_1_2_1.pt\n",
      "F1-score on test set is 0.8360981245672833\n",
      "Accuracy score on test set is 0.8294243070362474\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_angry_2_4_1_2_2.pt\n",
      "F1-score on test set is 0.8501528535746127\n",
      "Accuracy score on test set is 0.8443496801705757\n",
      "-------------\n",
      "Audio feature dimension is: 74\n",
      "Visual feature dimension is: 35\n",
      "Text feature sequence length is: 20\n",
      "Text feature dimension is: 300\n",
      "../history/tt_fusion_layer_head/model_lmf_neutral_2_4_1_2_0.pt\n",
      "F1-score on test set is 0.6252360719378324\n",
      "Accuracy score on test set is 0.6673773987206824\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_neutral_2_4_1_2_1.pt\n",
      "F1-score on test set is 0.6263682030023359\n",
      "Accuracy score on test set is 0.6609808102345416\n",
      "-------------\n",
      "../history/tt_fusion_layer_head/model_lmf_neutral_2_4_1_2_2.pt\n",
      "F1-score on test set is 0.6668572896157239\n",
      "Accuracy score on test set is 0.6780383795309168\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor use_LSTM in use_LSTM_list:\\n  if use_LSTM == 0:\\n    signiture = \\'LSTM_0\\'\\n  else:\\n    signiture = \\'ATTN_0\\'\\n  for max_rank in max_rank_list:\\n    for fusion_rank in fusion_rank_list:\\n      for run in range(runs):\\n        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\\n        print(model_path)\\n        acc = eval_at_model_path(model_path, test_iterator)\\n        print(\"-------------\")\\n        if acc > best_acc:\\n            best_acc = acc\\n            best_model_path = model_path\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = 0\n",
    "best_acc = 0\n",
    "# for max_rank in max_rank_list:\n",
    "\n",
    "signiture = 'lmf'\n",
    "model_path_dir = '../history/tt_fusion_layer_head'\n",
    "emotion_list = ['happy','sad','angry','neutral']\n",
    "max_rank = 2\n",
    "fusion_rank = 4\n",
    "n_layers = 1\n",
    "n_head = 2\n",
    "runs = 3\n",
    "\n",
    "for emotion in emotion_list:\n",
    "  train_set, valid_set, test_set, input_dims = load_iemocap(data_path, emotion)\n",
    "  test_iterator = DataLoader(test_set, batch_size=len(test_set), num_workers=2, shuffle=True)\n",
    "  # for fusion_rank in fusion_rank_list:\n",
    "  for run in range(runs):\n",
    "    model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "    print(model_path)\n",
    "    acc = eval_at_model_path(model_path, test_iterator)\n",
    "    print(\"-------------\")\n",
    "    if acc > best_acc:\n",
    "      best_acc = acc\n",
    "      best_model_path = model_path\n",
    "'''\n",
    "for lr in lr_list:\n",
    "  for decay in decay_list:\n",
    "    for run in range(runs):\n",
    "      model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,lr,decay,run))\n",
    "'''\n",
    "'''\n",
    "for max_rank in max_rank_list:\n",
    "  for fusion_rank in fusion_rank_list:\n",
    "    for run in range(runs):\n",
    "      model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "'''\n",
    "'''\n",
    "for TT_FUSION in TT_FUSION_list:\n",
    "  if TT_FUSION == 0:\n",
    "    signiture = 'attn_0'\n",
    "    lr = 0.0005\n",
    "    decay = 0.01\n",
    "  else:\n",
    "    signiture = 'attn_1'\n",
    "    lr = 0.003\n",
    "    decay = 0.002\n",
    "  for max_rank in max_rank_list:\n",
    "    for fusion_rank in fusion_rank_list:\n",
    "      for run in range(runs):\n",
    "        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "'''\n",
    "'''\n",
    "for use_LSTM in use_LSTM_list:\n",
    "  if use_LSTM == 0:\n",
    "    signiture = 'LSTM_0'\n",
    "  else:\n",
    "    signiture = 'ATTN_0'\n",
    "  for max_rank in max_rank_list:\n",
    "    for fusion_rank in fusion_rank_list:\n",
    "      for run in range(runs):\n",
    "        model_path = os.path.join(model_path_dir, \"model_{}_{}_{}_{}_{}_{}_{}.pt\".format(signiture, emotion,max_rank,fusion_rank,n_layers,n_head,run))\n",
    "        print(model_path)\n",
    "        acc = eval_at_model_path(model_path, test_iterator)\n",
    "        print(\"-------------\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_path = model_path\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZKOEKumibEl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WeJ9bXYeTieb",
    "r79jMnY-YdxX",
    "IP-WBBu3TuCf"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://gist.github.com/ngrislain/c3ba6f687c64ce31adc6b0dff1b26d6a#file-py38-success-ipynb",
     "timestamp": 1666743016140
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "name": "py38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
